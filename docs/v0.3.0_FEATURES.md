# Fennec v0.3.0 Features Guide

Complete guide to production-grade features in Fennec v0.3.0.

## Table of Contents

1. [Redis Caching](#redis-caching)
2. [Database Migrations](#database-migrations)
3. [Monitoring Tools](#monitoring-tools)
4. [Admin Dashboard](#admin-dashboard)
5. [Message Queues](#message-queues)
6. [gRPC Support](#grpc-support)

---

## Redis Caching

High-performance caching layer with multiple strategies.

### Installation

```bash
pip install redis hiredis
docker run -d -p 6379:6379 redis
```

### Basic Usage

```python
from fennec.cache import RedisCache

cache = RedisCache(url="redis://localhost:6379", prefix="myapp:")

# Set value
await cache.set("user:123", {"name": "Alice"}, ttl=300)

# Get value
user = await cache.get("user:123")

# Delete value
await cache.delete("user:123")

# Check existence
exists = await cache.exists("user:123")

# Clear pattern
await cache.clear("user:*")
```

### Cache Decorator

```python
from fennec.cache import cache, RedisCache

redis = RedisCache()

@cache(ttl=600, backend=redis)
async def get_user(user_id: int):
    # Expensive operation
    return await database.query(user_id)

# First call: cache miss (slow)
user = await get_user(123)

# Second call: cache hit (fast)
user = await get_user(123)
```

### Caching Strategies

#### Cache-Aside (Lazy Loading)

```python
from fennec.cache import CacheAside

cache_aside = CacheAside(redis, ttl=300)

async def get_user(user_id: int):
    return await cache_aside.get(
        f"user:{user_id}",
        loader=lambda: database.get_user(user_id)
    )
```

#### Write-Through

```python
from fennec.cache import WriteThrough

write_through = WriteThrough(redis, ttl=300)

async def save_user(user_data: dict):
    await write_through.set(
        f"user:{user_data['id']}",
        user_data,
        writer=lambda v: database.save_user(v)
    )
```

### Cache Statistics

```python
stats = await redis.get_stats()
print(f"Hit rate: {stats['hit_rate']}%")
print(f"Total requests: {stats['total_requests']}")
```

---

## Database Migrations

Version-controlled database schema management.

### Installation

```bash
pip install asyncpg  # For PostgreSQL
```

### Setup

```python
from fennec.migrations import MigrationManager

manager = MigrationManager(
    connection=db_connection,
    migrations_dir="migrations"
)

await manager.init()
```

### Create Migration

```python
# Create Python migration
filepath = await manager.create("add users table", migration_type="python")

# Create SQL migration
filepath = await manager.create("add index", migration_type="sql")
```

### Migration File Example

```python
from fennec.migrations import PythonMigration

class Migration20240104120000(PythonMigration):
    def __init__(self):
        super().__init__(
            version="20240104120000",
            description="add users table"
        )
    
    async def up(self, connection):
        await connection.execute("""
            CREATE TABLE users (
                id SERIAL PRIMARY KEY,
                username VARCHAR(50) UNIQUE NOT NULL,
                email VARCHAR(100) UNIQUE NOT NULL
            )
        """)
    
    async def down(self, connection):
        await connection.execute("DROP TABLE users")

migration = Migration20240104120000()
```

### Apply Migrations

```python
# Apply all pending migrations
count = await manager.migrate()

# Apply to specific version
count = await manager.migrate(target="20240104120000")
```

### Rollback

```python
# Rollback last migration
count = await manager.rollback(steps=1)

# Rollback multiple
count = await manager.rollback(steps=3)
```

### Check Status

```python
status = await manager.status()
print(f"Applied: {status['applied']}")
print(f"Pending: {status['pending']}")
```

---

## Monitoring Tools

Production-ready observability with Prometheus, tracing, and logging.

### Installation

```bash
pip install prometheus-client
```

### Prometheus Metrics

```python
from fennec.monitoring import PrometheusMetrics, MetricsMiddleware

metrics = PrometheusMetrics(app_name="my_app")
app.add_middleware(MetricsMiddleware(metrics))

# Expose metrics endpoint
@app.get("/metrics")
async def metrics_endpoint(request):
    return Response(
        metrics.generate_metrics(),
        content_type=metrics.get_content_type()
    )
```

### Available Metrics

- `http_requests_total` - Total HTTP requests
- `http_request_duration_seconds` - Request duration histogram
- `http_requests_active` - Active requests gauge
- `http_errors_total` - Total errors
- `websocket_connections_active` - Active WebSocket connections
- `cache_hits_total` / `cache_misses_total` - Cache performance
- `db_query_duration_seconds` - Database query duration

### Distributed Tracing

```python
from fennec.monitoring import RequestTracer, TracingMiddleware

tracer = RequestTracer(service_name="my_app")
app.add_middleware(TracingMiddleware(tracer))

# Manual span creation
span_id = tracer.start_span("database.query", attributes={"query": "SELECT"})
# ... do work ...
tracer.end_span(span_id)
```

### Structured Logging

```python
from fennec.monitoring import StructuredLogger

logger = StructuredLogger(name="my_app")

# Log with context
logger.info("User logged in", user_id=123, ip="192.168.1.1")

# Log request
logger.log_request("GET", "/users", 200, 0.123)

# Log error
logger.log_error(exception, context={"user_id": 123})
```

### Health Checks

```python
from fennec.monitoring.health import HealthCheck

health = HealthCheck(service_name="my_app")

# Add checks
health.add_check("database", check_database_connection)
health.add_check("cache", check_redis_connection)

# Run checks
@app.get("/health")
async def health_check(request):
    result = await health.run_checks()
    status = 200 if result['status'] == 'healthy' else 503
    return Response(result, status=status)
```

---

## Admin Dashboard

Web-based monitoring and management interface.

### Setup

```python
from fennec.admin import AdminDashboard

admin = AdminDashboard(
    app,
    auth_required=True,
    prefix="/admin"
)
```

### Features

- **Real-time Metrics**: Request rate, error rate, response times
- **System Monitoring**: CPU, memory, disk usage
- **Request History**: Recent requests with status and duration
- **Auto-refresh**: Dashboard updates every 2 seconds

### Access

Visit `http://localhost:8000/admin` in your browser.

### Authentication

```python
def check_admin(request):
    token = request.headers.get('X-Admin-Token')
    return verify_admin_token(token)

admin = AdminDashboard(
    app,
    auth_required=True,
    auth_check=check_admin
)
```

### API Endpoints

- `GET /admin` - Dashboard UI
- `GET /admin/api/metrics` - Application metrics
- `GET /admin/api/system` - System metrics
- `GET /admin/api/realtime` - Real-time data

---

## Message Queues

Asynchronous task processing with multiple backends.

### Installation

```bash
# Redis backend
pip install redis

# RabbitMQ backend
pip install aio-pika

# AWS SQS backend
pip install aioboto3
```

### Setup

```python
from fennec.queue import QueueManager, Worker

# Initialize queue manager
queue = QueueManager(
    backend="redis",  # or "rabbitmq", "sqs"
    connection_url="redis://localhost:6379"
)
```

### Define Tasks

```python
@queue.task(queue_name="emails", max_retries=3, retry_delay=60)
async def send_email(to: str, subject: str, body: str):
    await email_service.send(to, subject, body)
    return {"status": "sent"}
```

### Enqueue Tasks

```python
# Enqueue for async execution
result = await send_email("user@example.com", "Hello", "Welcome!")

# Direct execution (bypass queue)
result = await send_email.run("user@example.com", "Hello", "Welcome!")
```

### Start Worker

```python
worker = Worker(
    queue_manager=queue,
    queue_names=["emails", "notifications"],
    concurrency=4  # 4 concurrent workers
)

await worker.start()
```

### Delayed Messages

```python
# Execute in 1 hour
await queue.publish(
    "emails",
    {"to": "user@example.com", "subject": "Reminder"},
    delay=3600
)
```

### Retry Logic

Tasks automatically retry on failure with exponential backoff:
- 1st retry: 60s delay
- 2nd retry: 120s delay
- 3rd retry: 180s delay

---

## gRPC Support

High-performance RPC for microservices.

### Installation

```bash
pip install grpcio grpcio-tools
```

### Define Service (.proto file)

```protobuf
syntax = "proto3";

service UserService {
  rpc GetUser (GetUserRequest) returns (User);
  rpc ListUsers (ListUsersRequest) returns (stream User);
}

message GetUserRequest {
  int32 id = 1;
}

message User {
  int32 id = 1;
  string username = 2;
  string email = 3;
}
```

### Generate Code

```bash
python -m grpc_tools.protoc \
  -I. \
  --python_out=. \
  --grpc_python_out=. \
  user.proto
```

### Implement Server

```python
from fennec.grpc import GRPCServer, rpc_method
import user_pb2_grpc

class UserServicer(user_pb2_grpc.UserServiceServicer):
    @rpc_method(log_requests=True)
    async def GetUser(self, request, context):
        user = get_user_by_id(request.id)
        return user_pb2.User(
            id=user['id'],
            username=user['username'],
            email=user['email']
        )

server = GRPCServer(host="0.0.0.0", port=50051)
server.add_service(user_pb2_grpc, UserServicer())
await server.start()
```

### Implement Client

```python
from fennec.grpc import GRPCClient

client = GRPCClient(host="localhost", port=50051)
await client.connect()

response = await client.call(
    user_pb2_grpc.UserServiceStub,
    'GetUser',
    user_pb2.GetUserRequest(id=1)
)

print(f"User: {response.username}")
```

### Streaming RPC

```python
# Server streaming
async def ListUsers(self, request, context):
    for user in get_all_users():
        yield user_pb2.User(...)

# Client streaming
async def UpdateUsers(self, request_iterator, context):
    async for request in request_iterator:
        update_user(request)
    return UpdateResponse(count=count)

# Bidirectional streaming
async def Chat(self, request_iterator, context):
    async for message in request_iterator:
        yield ChatMessage(...)
```

---

## Best Practices

### Caching
- Set appropriate TTL based on data volatility
- Monitor cache hit rates (aim for >80%)
- Use key prefixes for organization
- Implement cache warming for critical data

### Migrations
- Always test migrations in development first
- Keep migrations small and focused
- Write both up() and down() methods
- Never modify applied migrations

### Monitoring
- Expose metrics endpoint for Prometheus
- Use structured logging for better searchability
- Implement health checks for all dependencies
- Set up alerts for critical metrics

### Message Queues
- Use separate queues for different priorities
- Implement idempotent task handlers
- Monitor queue depth and processing time
- Set appropriate retry limits

### gRPC
- Use Protocol Buffers for type safety
- Implement proper error handling
- Add timeouts to prevent hanging
- Use streaming for large datasets

---

## Migration from v0.2.0

All v0.2.0 features remain fully compatible. To use v0.3.0 features:

1. Install new dependencies
2. Add Redis/RabbitMQ if needed
3. Configure monitoring endpoints
4. Optional: Add admin dashboard
5. Optional: Integrate message queues
6. Optional: Add gRPC services

No breaking changes!
